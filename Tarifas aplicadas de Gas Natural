# Importamos las librerías necesarias
from pyspark.sql import SparkSession
from pyspark.sql import functions as F

# Inicializa la sesión de Spark
spark = SparkSession.builder.appName('Tarea3').getOrCreate()

# Define la ruta del archivo .csv en HDFS
# https://www.datos.gov.co/resource/ek3f-5wn4.csv
file_path = 'hdfs://localhost:9000/Tarea3/ek3f-5wn4.csv'

# Lee el archivo .csv
df = spark.read.format('csv').option('header', 'true').option('inferSchema', 'true').load(file_path)

# Seleccionamos solo las columnas requeridas para mostrar el esquema
df_selected = df.select("ANO", "MES", "NIT", "ESTRATO", "CARGO_FIJO")

# Imprimimos el esquema de las columnas seleccionadas
df_selected.printSchema()

# Seleccionamos el top 20 de valores más altos en "CARGO_FIJO" y mostramos las columnas requeridas
top_20_cargo_fijo = df_selected.orderBy(F.col("CARGO_FIJO").desc()).limit(20)

# Muestra el resultado
print("Top 20 del Cargo Fijo más alto con el mes, año, NIT y estrato correspondiente:\n")
top_20_cargo_fijo.show()

# Ordenar filas por los valores en la columna "CARGO_FIJO" en orden descendente con todos los campos
print("Valores ordenados de mayor a menor\n") 
sorted_df = df.sort(F.col("CARGO_FIJO").desc())
sorted_df.show()

# Finaliza la sesión de Spark
spark.stop()
